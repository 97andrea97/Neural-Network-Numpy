{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\97and\\miniconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:86: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NN tested for different applications such as:\n",
    "- classify a number as odd or even, giving the output in two different ways\n",
    "- detect the possibility of having an heart disease given some medical data\n",
    "- classify images representing handwritten digits.\n",
    "\n",
    "version regularized (L2 regularization, parameter \"lmbda\"), clever weight inizialization, momentum learning(parameter \"mu\"), tanh, and my.\n",
    "\n",
    "My idea: in order to equilibrate the multiplicative factor which slows down the learning of the first layers and\n",
    "causes the \"vanishing gradient\" I use a parameter called \"my\" which multiplies delta at each layer.\n",
    "It I think to have the gradient vanishing problem I set my>1, If I think to have the gradient exploding problem I set my<1. \n",
    "It seems to work.\n",
    "\n",
    "'''\n",
    "\n",
    "################################ libraries:\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "################################ cost and activation functions:\n",
    "\n",
    "# cost functions:\n",
    "\n",
    "class quadratic_cost:\n",
    "    def fn(outputs,labels):\n",
    "        return 0.5*np.linalg.norm(outputs-labels)**2  # 0.5 * (two_norm)^2\n",
    "    \n",
    "    def delta_L(outputs,labels,z_L,activation_function): # returns the delta error of the output layer\n",
    "        nabla_aC = np.add(outputs, -labels) \n",
    "        sigma_der = np.array([activation_function.derivative(z_L_i) for z_L_i in z_L])\n",
    "        delta = np.multiply(nabla_aC, sigma_der)\n",
    "        return delta\n",
    "        \n",
    "\n",
    "class cross_entropy_cost:\n",
    "    def fn(outputs,labels):\n",
    "        return np.sum(np.nan_to_num(-labels*np.log(outputs)-(1-labels)*np.log(1-outputs)))\n",
    "    \n",
    "    def delta_L(outputs,labels,z_L,activation_function):  # returns the delta error of the output layer\n",
    "        return np.add(outputs, -labels)\n",
    "       \n",
    "# activation functions:\n",
    "\n",
    "class sigmoid:\n",
    "    def fn(z):\n",
    "        return 1.0 /(1.0 + math.exp(-z))\n",
    "    def derivative(z):\n",
    "        return  sigmoid.fn(z)*(1-sigmoid.fn(z))\n",
    "\n",
    "    \n",
    "class tanh:\n",
    "    def fn(z):\n",
    "        return (math.exp(z) - math.exp(-z))/(math.exp(z) + math.exp(-z))\n",
    "                                             \n",
    "    def derivative(z):\n",
    "        return 1 - tanh.fn(z)**2\n",
    "\n",
    "################################ Neural Network definition:        \n",
    "\n",
    "\n",
    "class NN:\n",
    "    def __init__(self, layer_dimensions):\n",
    "    \n",
    "        self.L = len(layer_dimensions)\n",
    "        self.weights = [ np.random.randn(layer_dimensions[ i + 1 ], layer_dimensions[i])/ np.sqrt(layer_dimensions[i]) for i in range(self.L - 1) ]\n",
    "        self.biases = [ np.zeros(layer_dimensions[ i + 1 ]) for i in range(self.L - 1) ]\n",
    "        self.z = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        self.deltas = [ np.zeros(b.shape) for b in self.biases ]\n",
    "        self.activations = [ np.zeros(layer_dimensions[i]) for i in range(self.L) ]\n",
    "        self.dC_dW = [ np.zeros(W.shape) for W in self.weights ]\n",
    "     \n",
    "    def compute_mean_cost(self,dataset, len_tset,cost_function, activation_function, lmbda):\n",
    "        \n",
    "        mean_cost = 0\n",
    "        for ex in dataset:\n",
    "                self.feedforward(ex[0],activation_function)\n",
    "                self.activations[-1]=np.array(self.activations[-1])\n",
    "                cost = cost_function.fn(self.activations[-1],ex[1]) \n",
    "                mean_cost += cost\n",
    "        mean_cost /= len(dataset)\n",
    "        \n",
    "        regularization_term = lmbda/(2*len_tset) * np.sum(np.sum(W**2) for W in net.weights)\n",
    "        return mean_cost + regularization_term\n",
    "    \n",
    " \n",
    "    def compute_mean_accuracy(self,dataset,activation_function):\n",
    "        mean_accuracy = 0\n",
    "        for ex in dataset:\n",
    "                self.feedforward(ex[0], activation_function)\n",
    "                output = self.activations[-1]\n",
    "                output = np.array(output)\n",
    "                if len(output)==1:\n",
    "                    output = output >0.5\n",
    "                    accuracy = np.mean(output == ex[1])\n",
    "                else:\n",
    "                    accuracy = np.mean(np.argmax(output)==np.argmax(ex[1]))\n",
    "                mean_accuracy += accuracy\n",
    "        mean_accuracy /= len(dataset)\n",
    "        return mean_accuracy\n",
    "    \n",
    "   \n",
    "    def learning(self, training_dataset, test_dataset, show_results, batch_size = 1, learning_rate = 0.1 , n_epoches = 10, activation_function=sigmoid, cost_function = quadratic_cost, lmbda= 0, mu=0, my=1):\n",
    "        \n",
    "        len_tset = training_dataset.shape[0]\n",
    "        \n",
    "        #compute cost and accuracy before learning:\n",
    "        cost=[]\n",
    "        cost.append(self.compute_mean_cost(training_dataset, len_tset, cost_function,activation_function, lmbda))\n",
    "        accuracy=[]\n",
    "        accuracy.append(self.compute_mean_accuracy(test_dataset,activation_function))\n",
    "        \n",
    "        #start learning:\n",
    "        epoch = 1\n",
    "        velocity = [ np.zeros(W.shape) for W in self.weights ]\n",
    "        while (epoch <= n_epoches):    \n",
    "                \n",
    "            training_dataset = np.random.permutation(training_dataset) #in order to create different batches at each epoch\n",
    "            batches = [ training_dataset[i:i+batch_size] for i in range(0, len(training_dataset), batch_size) ]\n",
    "            \n",
    "            for batch in batches:  # for each batch\n",
    "                \n",
    "                \n",
    "                mean_dC_dW = [ np.zeros(W.shape) for W in self.weights ]\n",
    "                mean_dC_db = [ np.zeros(b.shape) for b in self.biases ]\n",
    "                \n",
    "                for ex in batch: # for each training example\n",
    "                    \n",
    "                    self.feedforward(ex[0],activation_function)\n",
    "                    self.backprop(ex[1], cost_function,activation_function, my)\n",
    "   \n",
    "                    mean_dC_dW = [np.add(mean_dC_dW[l], self.dC_dW[l]) for l in range(len(self.dC_dW)) ]\n",
    "                    mean_dC_db = [np.add(mean_dC_db[l], self.deltas[l]) for l in range(len(self.deltas)) ]\n",
    "                    \n",
    "                # update weights:\n",
    "                weights_decay = 1 - learning_rate*lmbda/len_tset\n",
    "                \n",
    "                velocity =  [np.add(mu*velocity[l], -learning_rate*mean_dC_dW[l]/batch_size) for l in range(len(self.dC_dW)) ]\n",
    "                \n",
    "                self.weights = [ np.add(self.weights[l]* weights_decay, velocity[l]) for l in range(len(self.weights)) ]              \n",
    "                # update biases:\n",
    "                self.biases = [ np.add(self.biases[l], -1*(mean_dC_db[l]*learning_rate)/batch_size) for l in range(len(self.biases)) ]  \n",
    "                \n",
    "            # compute results:\n",
    "            if show_results:\n",
    "                cost.append(self.compute_mean_cost(training_dataset, len_tset, cost_function, activation_function,lmbda))   \n",
    "                accuracy.append(self.compute_mean_accuracy(test_dataset, activation_function))\n",
    "                \n",
    "            elif epoch==n_epoches:\n",
    "                print(\"final accuracy:\", self.compute_mean_accuracy(test_dataset, activation_function))\n",
    "                \n",
    "            epoch+=1\n",
    "        \n",
    "        \n",
    "        # show results:\n",
    "        \n",
    "        if show_results:\n",
    "            fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "            ax1.scatter(range(len(cost)), cost, color='r', s=2)\n",
    "            ax1.set_title(\"COST\")\n",
    "            ax2.scatter(range(len(accuracy)), accuracy, color='b', s=2)\n",
    "            ax2.set_title(\"ACCURACY\")\n",
    "            plt.show()\n",
    "            print(\"final accuracy:\", accuracy[-1])\n",
    "\n",
    "            \n",
    "    def feedforward(self,input_data, activation_function):\n",
    "        \n",
    "        self.activations[0] = input_data\n",
    "        \n",
    "        for l in range(self.L-1):\n",
    "\n",
    "            self.z[l] = np.add( np.dot(self.weights[l],self.activations[l]), self.biases[l])\n",
    "            self.activations[l+1] = [activation_function.fn(z_l_i) for z_l_i in self.z[l] ]\n",
    "    \n",
    "    \n",
    "\n",
    "    def backprop(self, labels, cost_function, activation_function, my):\n",
    "        \n",
    "        # BP1: to cmpute the delta_error at the output layer = delta_L = 𝛿_L =∇_𝑎𝐶 ⊙ 𝜎'(𝑧_𝐿) \n",
    "        \n",
    "        self.deltas[-1] = cost_function.delta_L(self.activations[-1],labels, self.z[-1], activation_function)\n",
    "        \n",
    "\n",
    "        # BP2: to backpropagate the error and compute delta_error at each level = delta_l = 𝛿_l = ( 𝑊_(l+1)*𝛿_(l+1) ) ⊙ 𝜎′(𝑧_𝑙):\n",
    "        for l in range(2,self.L):\n",
    "            \n",
    "            \n",
    "            W_times_delta = np.dot(np.transpose(self.weights[-l+1]), self.deltas[-l+1])\n",
    "            \n",
    "\n",
    "            sigma_der = np.array([activation_function.derivative(z_l_i) for z_l_i in self.z[-l]])\n",
    "\n",
    "\n",
    "            self.deltas[-l] = my*np.multiply(W_times_delta, sigma_der)\n",
    "            # my is my parameter, I think this can solve in some cases the vanishing gradient problem.\n",
    "                  \n",
    "            \n",
    "        # BP3: states that dC/db is exactly delta. \n",
    "        # So i don't need to do further computation\n",
    "        \n",
    "        \n",
    "        # BP4: states that dC/dw_l = 𝛿_l(a(l-1))^T:\n",
    "        for l in range(1,self.L):\n",
    "            \n",
    "            self.dC_dW[-l] = np.dot(np.expand_dims(self.deltas[-l],axis=1),np.expand_dims(self.activations[-l-1],axis=0))\n",
    "            \n",
    "\n",
    "\n",
    "################################ Datasets:\n",
    "\n",
    "# data= 0 odd-even ill-posed problem dataset  data=1 to use the odd-even number dataset\n",
    "# data=2 to use the heartdisease dataset    # data=3 to use handwritten digit images\n",
    "\n",
    "\n",
    "def create_data(data,n_ex):\n",
    "\n",
    "    if data == 0: # data= 0 odd-even ill-posed problem dataset\n",
    "        \n",
    "        n = n_ex #number of dataset examples\n",
    "        n_in=1 #number of input neurons\n",
    "        n_out=1 #number of output neurons\n",
    "\n",
    "        dataset = [[np.array([num/100]),np.array(num%2)] for num in np.random.randint(1,10, size=n)]\n",
    "\n",
    "    \n",
    "    if data == 1:  # data=1 to use the odd-even number dataset\n",
    "        \n",
    "        n = n_ex   #number of dataset examples\n",
    "        n_in=3 #number of input neurons = maximum number value which can be given as input\n",
    "        n_out=2  #number of output neurons\n",
    "\n",
    "        numbers = np.random.randint(1,n_in, size=n)\n",
    "        training_inputs = []\n",
    "        training_labels = []\n",
    "        for num in numbers:   # simple binary codification: 3 corresponds to [ 0,0,1,...]\n",
    "            inp = np.zeros(n_in)\n",
    "            out = np.zeros(n_out)\n",
    "            inp[num-1]=1\n",
    "            out[num%2]=1\n",
    "            training_inputs.append(inp)\n",
    "            training_labels.append(out)\n",
    "\n",
    "        dataset = np.array([[training_inputs[i],training_labels[i]] for i in range(len(training_labels))])\n",
    "\n",
    "    if data == 2:  # data=2 to use the heartdisease dataset\n",
    "        import pandas as pd\n",
    "        dataset=pd.read_csv(\"heart.csv\");\n",
    "\n",
    "        n = dataset.shape[0]      # number of data examples: 303 max\n",
    "        n_in = dataset.shape[1]   #number of input neurons\n",
    "        n_out=1                   #number of output neurons\n",
    "        \n",
    "    \n",
    "        inputs=dataset.iloc[:,:n_in]\n",
    "        #since the dataset contains only positive values I can normalize it dividing each column by the maximum value of each column.:\n",
    "        inputs = inputs / inputs.max()\n",
    "        labels=dataset.iloc[:,-1]\n",
    "        dataset = [[np.array(inputs.iloc[i]), np.array(labels.iloc[i])] for i in range(n)]\n",
    "\n",
    "\n",
    "    \n",
    "    if data == 3: # data=3 to use handwritten digit images\n",
    "        \n",
    "        n=n_ex      \n",
    "        n_in=784     #number of input neurons\n",
    "        n_out=10     #number of output neurons\n",
    "    \n",
    "        dataset = np.load('mini-mnist.npy', allow_pickle=True)\n",
    "\n",
    "    # training and test set creation:\n",
    "    dataset = np.random.permutation(dataset)\n",
    "    training_dataset = dataset[:round(n-n/10)]\n",
    "    test_dataset = dataset[round(n-n/10)+1:n]\n",
    "    return training_dataset,test_dataset,n,n_in,n_out\n",
    "\n",
    "\n",
    "################################ NN learning call:\n",
    "# data= 0 odd-even ill-posed problem dataset  data=1 to use the odd-even number dataset\n",
    "# data=2 to use the heartdisease dataset    # data=3 to use handwritten digit images\n",
    "\n",
    "# to replicate random experiments:\n",
    "np.random.seed(4)\n",
    "\n",
    "training_set,test_set,n,n_in,n_out = create_data(data=2,n_ex=1000)\n",
    "net = NN([n_in,10,n_out])   \n",
    "net.learning(training_set, training_set, show_results=True, batch_size=round(n/10), learning_rate=0.3, n_epoches = 1000, activation_function = sigmoid, cost_function = cross_entropy_cost , lmbda=5, mu = 0.4, my=3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
